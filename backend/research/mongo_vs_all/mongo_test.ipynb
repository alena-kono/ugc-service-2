{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mongo vs ClickHouse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "\n",
    "test_comment = \" \".join([rd.choice([\"lorem\", \"ipsum\", \"dolor\", \"sit\", \"amet\", \"consectetur\", \"adipiscing\", \"elit\", \"sed\", \"do\", \"eiusmod\", \"tempor\", \"incididunt\", \"ut\", \"labore\", \"et\", \"dolore\", \"magna\", \"aliqua\"]) for _ in range(50)])\n",
    "test_comment = test_comment.capitalize() + \".\"\n",
    "test_comment = test_comment * 6\n",
    "test_comment = test_comment[:300]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "from random import choice\n",
    "\n",
    "users = [str(uuid4()) for _ in range(100)]\n",
    "films = [str(uuid4()) for _ in range(1000)]\n",
    "\n",
    "def generate_review_batch()->list[dict]:\n",
    "    for i in range(10_000):\n",
    "        i % 100 == 0 and print(f\"{i} / 10_000\")\n",
    "        batch = [\n",
    "            {\n",
    "                \"user_id\": choice(users),\n",
    "                \"film_id\": choice(films),\n",
    "                \"comment\": test_comment,\n",
    "                \"timestamp\": datetime.now(),\n",
    "            }\n",
    "            for _ in range(1000)\n",
    "        ]\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost', 27019)\n",
    "db = client['test_database']\n",
    "collection = db['test_collection']\n",
    "collection.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.create_index(\"user_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "counter = 0\n",
    "for batch in generate_review_batch():\n",
    "    collection.insert_many(batch) \n",
    "    counter += 1\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Batch {counter} inserted\")\n",
    "print(f\"Insertion took {end - start:.4f} seconds\")\n",
    "print(f\"average insertion time: {(end - start) / counter:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "def read_test():\n",
    "    cursor  = collection.find().batch_size(1000).limit(1000)\n",
    "    _ = list(cursor)\n",
    "\n",
    "print(f\"Average select time: {timeit.timeit(read_test, number=1000) / 1000} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "pipeline=[\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$user_id\",\n",
    "            \"count\": {\"$sum\": 1}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$sort\": {\"count\": -1}\n",
    "    },\n",
    "    {\n",
    "        \"$limit\": 10\n",
    "    }\n",
    "]\n",
    "def aggregate_test():\n",
    "    cursor = collection.aggregate(pipeline=pipeline)\n",
    "    list(cursor)\n",
    "\n",
    "print(f\"Average select time: {timeit.timeit(aggregate_test, number=10) / 10} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert batch test:\n",
    "* Batch size = 1000.  10000 batches had been inserted\n",
    "* Insertion took 169.2879 seconds\n",
    "* average insertion time: 0.0169 seconds\n",
    "\n",
    "### Read batch test\n",
    "* Batch size = 1000.\n",
    "* Reads Number = 1000 times \n",
    "* Average select time: 0.006814309166977182 sec\n",
    "\n",
    "## Aggregation test\n",
    "* Reads Number = 10 times \n",
    "* Average aggregate time: 3.495923020900227 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ClickHouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clickhouse_driver import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(host=\"localhost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.execute('CREATE DATABASE IF NOT EXISTS collection ON CLUSTER company_cluster;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.execute(\n",
    "\"\"\"CREATE TABLE IF NOT EXISTS collection.test_collection ON CLUSTER company_cluster (\n",
    "    user_id UUID,\n",
    "    film_id UUID,\n",
    "    comment TEXT,\n",
    "    timestamp TIMESTAMP\n",
    ")\n",
    "Engine=MergeTree()\n",
    "ORDER BY (user_id, film_id, timestamp);\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "counter = 0\n",
    "for batch in generate_review_batch():\n",
    "    client.execute(\"INSERT INTO collection.test_collection (user_id, film_id, comment, timestamp) VALUES\", batch)\n",
    "    counter += 1\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Batch {counter} inserted\")\n",
    "print(f\"Insertion took {end - start:.4f} seconds\")\n",
    "print(f\"average insertion time: {(end - start) / counter:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "def read_test():\n",
    "    _ = client.execute(\"SELECT * FROM collection.test_collection LIMIT 1000\")\n",
    "\n",
    "print(f\"Average select time: {timeit.timeit(read_test, number=1000) / 1000} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_pipeline = \"\"\"SELECT user_id, COUNT(*) as count FROM collection.test_collection GROUP BY user_id ORDER BY count DESC LIMIT 10 \"\"\"\n",
    "def aggregate_test():\n",
    "    _ = client.execute(sql_pipeline)\n",
    "\n",
    "print(f\"Average select time: {timeit.timeit(aggregate_test, number=10) / 10} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert batch test:\n",
    "* Batch size = 1000.  10000 batches had been inserted\n",
    "* Insertion took 233.7237 seconds\n",
    "* average insertion time: 0.0234 seconds\n",
    "\n",
    "### Read batch test\n",
    "* Batch size = 1000.\n",
    "* Reads Number = 1000 times \n",
    "* Average select time: 0.012428301582986023 sec\n",
    "\n",
    "## Aggregation test\n",
    "* Reads Number = 10 times \n",
    "* Average select time: 0.17657093749730848 sec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mongo-vs-all-ismdL_0K-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
